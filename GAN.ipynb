{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = tfds.load(\n",
    "      \"mnist\",\n",
    "      with_info=True,\n",
    "      split=\"train\"\n",
    "    )\n",
    "\n",
    "tfds.show_examples(data, info, plot_scale=2.0, rows=3, cols=6)\n",
    "\n",
    "print(f\"\\nFeatures: {info.features}\")\n",
    "print(f\"Loaded examples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add model layers\n",
    "def extract_and_normalize(item):\n",
    "  image = tf.cast(item[\"image\"], tf.float32)\n",
    "  image = (image - 127.5) / 127.5  # mapping pixel values to range [-1;1]\n",
    "  return image\n",
    "\n",
    "train_images = data.map(extract_and_normalize) \\\n",
    "  .cache() \\\n",
    "  .shuffle(buffer_size=2048) \\\n",
    "  .batch(256) \\\n",
    "  .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Number of batches: {len(train_images)}\")\n",
    "print(f\"Element spec: {train_images.element_spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = tf.keras.Sequential([\n",
    "  \n",
    "  # add model layers\n",
    "  tf.keras.layers.Input(shape=(100,)),\n",
    "  tf.keras.layers.Dense(7*7*256),  # Adjust the size to match the target shape\n",
    "  tf.keras.layers.Reshape(target_shape=(7, 7, 256)),\n",
    "  tf.keras.layers.UpSampling2D(),\n",
    "  tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.UpSampling2D(),\n",
    "  tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.Conv2D(filters=1, kernel_size=3, padding='same', activation='tanh')\n",
    "], name=\"generator\")\n",
    "\n",
    "def output_model(data_element):\n",
    "    image = data_element\n",
    "    greyscale = tf.image.rgb_to_grayscale(image)\n",
    "    greyscale = tf.image.resize(greyscale, size=[28,28,1])\n",
    "    return greyscale\n",
    "\n",
    "generator.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator model\n",
    "discriminator = tf.keras.Sequential()\n",
    "discriminator.add(tf.keras.layers.Input(shape=[28,28,1]))\n",
    "discriminator.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "discriminator.add(tf.keras.layers.MaxPooling2D())\n",
    "discriminator.add(tf.keras.layers.Flatten())\n",
    "discriminator.add(tf.keras.layers.Dense(units=2500))\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for calculating cross entropy\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# note: input parameters (decisions) are discriminator verdicts: \"are images real or fake?\"\n",
    "\n",
    "def generator_loss(decisions):\n",
    "  expected = tf.ones_like(decisions) # we expect all generated images are recognized as \"real\" (all ones)\n",
    "  return cross_entropy(expected, decisions)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_decisions, fake_decisions):\n",
    "  real_expected = tf.ones_like(real_decisions) # we expect real images are recognized as \"real\" (all ones)\n",
    "  real_loss = cross_entropy(real_expected, real_decisions)\n",
    "  fake_expected = tf.zeros_like(fake_decisions) # and we expect fake images are recoginized as \"fake\" (all zeros)\n",
    "  fake_loss = cross_entropy(fake_expected, fake_decisions)\n",
    "  return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "  batch_size = len(real_images)\n",
    "  gen_inputs = tf.random.normal(shape=(batch_size, 100))\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    fake_images = generator(gen_inputs, training=True)\n",
    "\n",
    "    real_decisions = discriminator(real_images, training=True)\n",
    "    fake_decisions = discriminator(fake_images, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_decisions)\n",
    "    disc_loss = discriminator_loss(real_decisions, fake_decisions)\n",
    "\n",
    "  gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "  gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "\n",
    "  disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "  disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "\n",
    "  return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_inputs = tf.random.normal(shape=(6, 100))\n",
    "\n",
    "for epoch in range(25):\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    for batch in train_images:\n",
    "        gen_loss, disc_loss = train_step(batch)\n",
    "        gen_losses.append(gen_loss)\n",
    "        disc_losses.append(disc_loss)\n",
    "        \n",
    "        avg_gen_loss = sum(gen_losses) / len(gen_losses)\n",
    "        avg_disc_loss = sum(disc_losses) / len(disc_losses)\n",
    "        print(f'\\rEpoch {epoch + 1}/{25} | Average Gen Loss: {avg_gen_loss} | Average Disc Loss: {avg_disc_loss}', end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
